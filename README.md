#  Student Score: Predicci贸n de Rendimiento Acad茅mico

![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python&logoColor=white)
![Algorithm](https://img.shields.io/badge/Algorithm-Gradient_Descent-red?logo=math&logoColor=white)
![Status](https://img.shields.io/badge/Status-Finalizado-green)

##  Descripci贸n del Proyecto
Este proyecto tiene como objetivo analizar la relaci贸n entre el **tiempo dedicado al estudio** y el **rendimiento acad茅mico**. 

A diferencia de implementaciones est谩ndar, en este desaf铆o se construy贸 un modelo de **Regresi贸n Lineal Simple** implementando el algoritmo de **Descenso de Gradiente** desde cero (manualmente), sin depender de la l贸gica de entrenamiento de librer铆as de "caja negra". Esto permite comprender matem谩ticamente c贸mo la m谩quina minimiza la funci贸n de costo para encontrar la l铆nea de mejor ajuste.

##  Dataset y An谩lisis Exploratorio (EDA)
El set de datos consta de 25 registros con dos variables:
*   **$X$ (Hours):** Horas de estudio.
*   **$y$ (Scores):** Calificaci贸n obtenida.

**Hallazgos del EDA:**
*   **Correlaci贸n:** Se detect贸 una correlaci贸n de Pearson de **0.98**, indicando una relaci贸n lineal positiva casi perfecta.
*   **Calidad de Datos:** Ausencia total de *outliers* (valores at铆picos) y una distribuci贸n limpia, lo que hace a este dataset ideal para modelos lineales.

## 锔 Metodolog铆a: Descenso de Gradiente
Se implement贸 el algoritmo iterativo para minimizar la funci贸n de costo (MSE).

*   **Learning Rate ($\alpha$):** 0.05
*   **Convergencia:** El costo inicial fue de **601.7** y descendi贸 hasta estabilizarse en **14.44** antes de la iteraci贸n 200.

### Ecuaci贸n Resultante
El modelo encontr贸 los siguientes par谩metros 贸ptimos:

$$ Score = 9.78 \cdot (Hours) + 2.48 $$

**Interpretaci贸n:**
1.  **Pendiente ($m \approx 9.78$):** Por cada hora adicional de estudio, el alumno aumenta su nota en casi **10 puntos**.
2.  **Intercepto ($b \approx 2.48$):** Un alumno que no estudie nada (0 horas) obtendr铆a una nota base de 2.5 puntos.

##  Visualizaci贸n de Resultados

### 1. Ajuste del Modelo
La l铆nea de regresi贸n (roja) corta los datos minimizando la distancia cuadr谩tica a los puntos reales.

<img src="images/regresion_lineal.png" alt="Gr谩fico de Regresi贸n Lineal" width="300"/>

### 2. Curva de Aprendizaje
Visualizaci贸n de c贸mo el algoritmo de Descenso de Gradiente minimiz贸 el error iteraci贸n tras iteraci贸n.

<img src="images/curva_costo.png" alt="Curva de Costo" width="300"/>

##  Evaluaci贸n de M茅tricas

| M茅trica | Valor | Interpretaci贸n |
| :--- | :--- | :--- |
| **$R^2$ (R-Cuadrado)** | **95.29%** | El modelo explica el 95% de la variabilidad de las notas bas谩ndose solo en las horas de estudio. |
| **RMSE** | **3.80** | El margen de error promedio de las predicciones es de +/- 3.8 puntos. |

##  Conclusiones
El experimento fue exitoso demostrando que:
1.  Existe una relaci贸n directa y cuantificable entre esfuerzo (tiempo) y resultado (nota).
2.  El algoritmo de **Descenso de Gradiente** implementado manualmente logr贸 converger a una soluci贸n 贸ptima con un error m铆nimo, validando la teor铆a matem谩tica detr谩s de la regresi贸n lineal.

---
**by:** Alejandro Barrenechea